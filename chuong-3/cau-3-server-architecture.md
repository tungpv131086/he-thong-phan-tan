CHÆ¯Æ NG 3 - CÃ‚U 3
Äá» bÃ i:
Trong má»™t há»‡ thá»‘ng phÃ¢n tÃ¡n phá»¥c vá»¥ Ä‘á»c nhiá»u â€“ ghi Ã­t (read-heavy, write-light) nhÆ° dá»‹ch vá»¥ táº­p tin, báº¡n sáº½ chá»n mÃ´ hÃ¬nh Ä‘a luá»“ng, Ä‘Æ¡n luá»“ng, hay mÃ¡y tráº¡ng thÃ¡i há»¯u háº¡n (event-driven FSM) cho thÃ nh pháº§n xá»­ lÃ½ trÃªn mÃ¡y chá»§? HÃ£y Ä‘Ã¡nh giÃ¡ dá»±a trÃªn cÃ¡c tiÃªu chÃ­: hiá»‡u nÄƒng, kháº£ nÄƒng má»Ÿ rá»™ng, Ä‘á»™ phá»©c táº¡p phÃ¡t triá»ƒn vÃ  kháº£ nÄƒng chá»‹u lá»—i.

BÃ€I GIáº¢I:
Pháº§n 1: PhÃ¢n tÃ­ch Ä‘áº·c Ä‘iá»ƒm cá»§a há»‡ thá»‘ng
A. Äáº·c Ä‘iá»ƒm workload: Read-heavy, Write-light
PhÃ¢n tÃ­ch tá»· lá»‡:
Typical file service workload:
- Read operations: 95% (metadata lookup, file read)
- Write operations: 5% (file create, update, delete)

Request characteristics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ READ requests (95%):                    â”‚
â”‚ - High frequency: 10,000 req/sec       â”‚
â”‚ - Low latency required: <10ms           â”‚
â”‚ - I/O bound: Disk read, network send   â”‚
â”‚ - Can be cached                         â”‚
â”‚ - Independent (no conflicts)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WRITE requests (5%):                    â”‚
â”‚ - Low frequency: 500 req/sec            â”‚
â”‚ - Can tolerate higher latency: <50ms    â”‚
â”‚ - Requires synchronization              â”‚
â”‚ - Must maintain consistency             â”‚
â”‚ - Potential conflicts                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
I/O pattern:
File read operation:
1. Lookup metadata (100Î¼s - cache hit)
2. Read from disk (5ms - if not cached)
3. Send to client (2ms - network)
Total: ~7ms

â†’ I/O dominant (CPU idle 90% of time)
â†’ Opportunity for concurrency!

B. YÃªu cáº§u há»‡ thá»‘ng
Performance Requirements:
âœ“ Throughput: 10,000+ concurrent read requests
âœ“ Latency: P95 < 10ms for reads
âœ“ Scalability: Linear scaling with cores
âœ“ Resource efficient: Minimal memory per connection

Fault Tolerance:
âœ“ Handle client disconnections gracefully
âœ“ Recover from partial failures
âœ“ No data corruption under load

Development:
âœ“ Maintainable codebase
âœ“ Easy to debug
âœ“ Reasonable development time

Pháº§n 2: MÃ´ hÃ¬nh 1 - Single-threaded (ÄÆ¡n luá»“ng)
A. Kiáº¿n trÃºc Single-threaded
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FILE SERVER PROCESS            â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Main Thread (single)           â”‚ â”‚
â”‚  â”‚                                  â”‚ â”‚
â”‚  â”‚   while (1) {                    â”‚ â”‚
â”‚  â”‚     client = accept();           â”‚ â”‚
â”‚  â”‚     request = read(client);      â”‚ â”‚
â”‚  â”‚     response = process(request); â”‚ â”‚
â”‚  â”‚     write(client, response);     â”‚ â”‚
â”‚  â”‚     close(client);               â”‚ â”‚
â”‚  â”‚   }                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Äáº·c Ä‘iá»ƒm:
- Sequential processing
- One request at a time
- Blocking I/O
Code example:
c// Single-threaded file server
void single_threaded_server() {
    int server_fd = socket(AF_INET, SOCK_STREAM, 0);
    bind(server_fd, ...);
    listen(server_fd, BACKLOG);
    
    while (1) {
        // Accept connection (BLOCKING)
        int client_fd = accept(server_fd, ...);
        
        // Read request (BLOCKING)
        Request req;
        read(client_fd, &req, sizeof(req));
        
        // Process request (BLOCKING on disk I/O)
        Response resp = handle_file_request(&req);
        
        // Send response (BLOCKING)
        write(client_fd, &resp, sizeof(resp));
        
        // Close connection
        close(client_fd);
        
        // Only now can we handle next request!
    }
}

Response handle_file_request(Request* req) {
    if (req->type == READ) {
        // Open file (disk I/O - BLOCKS)
        int fd = open(req->filename, O_RDONLY);
        
        // Read data (disk I/O - BLOCKS)
        char buffer[4096];
        read(fd, buffer, req->size);
        
        close(fd);
        
        return create_response(buffer);
    }
    // Similar for WRITE, DELETE, etc.
}
```

---

**B. PhÃ¢n tÃ­ch hiá»‡u nÄƒng (Performance)**

**1. Throughput - Ráº¤T THáº¤P âŒ**
```
Timeline for 3 requests:

Time â†’
0ms   10ms  20ms  30ms  40ms  50ms  60ms
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚ R1  â”‚     â”‚ R2  â”‚     â”‚ R3  â”‚     â”‚
â”‚accptâ”‚read â”‚accptâ”‚read â”‚accptâ”‚read â”‚
â”‚ â†“   â”‚disk â”‚ â†“   â”‚disk â”‚ â†“   â”‚disk â”‚
â”‚proc â”‚wait â”‚proc â”‚wait â”‚proc â”‚wait â”‚
â”‚send â”‚     â”‚send â”‚     â”‚send â”‚     â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Each request: 20ms (10ms processing + 10ms wait)
Throughput: 1 request / 20ms = 50 requests/second

Required: 10,000 requests/second
Shortfall: 200x too slow! âŒ
```

**2. CPU Utilization - Ráº¤T KÃ‰M âŒ**
```
CPU Usage breakdown:

Processing:    10% (actual compute)
Disk I/O wait: 70% (CPU IDLE) âŒ
Network wait:  15% (CPU IDLE) âŒ
Other:         5%

Total CPU usage: ~10%
Wasted capacity: 90% âŒ

Even on 8-core CPU:
- Only 1 core at 10% = 1.25% total utilization
- 7 cores completely idle
â†’ Massive waste!
```

**3. Latency - KÃ‰M âŒ**
```
Request latency:

Best case (no queue):
- Processing: 10ms
- Total: 10ms âœ“

With load (10 requests waiting):
- Queue time: 10 Ã— 20ms = 200ms
- Processing: 10ms
- Total: 210ms âŒ
- P95 latency: >200ms (20x requirement!)
```

---

**C. Kháº£ nÄƒng má»Ÿ rá»™ng (Scalability) - Ráº¤T KÃ‰M âŒ**
```
Scaling analysis:

Add more CPU cores:
- Single thread uses only 1 core
- Other cores idle
- NO IMPROVEMENT âŒ

Add more RAM:
- No effect (not memory-bound)
- NO IMPROVEMENT âŒ

Add more servers:
- Linear cost increase
- Each server: 50 req/sec
- Need 200 servers for 10K req/sec!
- Cost: $$$$ âŒ

Scalability score: 1/10 âŒ

D. Äá»™ phá»©c táº¡p phÃ¡t triá»ƒn (Development Complexity) - Tá»T âœ…
c// Code ráº¥t Ä‘Æ¡n giáº£n, dá»… hiá»ƒu

void handle_request() {
    // Tuáº§n tá»±, dá»… theo dÃµi
    Step 1: Accept connection
    Step 2: Read request
    Step 3: Process
    Step 4: Send response
    Step 5: Close
    
    // No synchronization needed âœ…
    // No race conditions âœ…
    // No deadlocks âœ…
}

// Debugging dá»… dÃ ng
// - Single execution path
// - gdb step-through works perfectly
// - printf debugging effective

Development time: 1 week âœ…
Complexity: LOW (2/10) âœ…
```

---

**E. Kháº£ nÄƒng chá»‹u lá»—i (Fault Tolerance) - TRUNG BÃŒNH âš ï¸**
```
Failure scenarios:

1. Client disconnect during processing:
   - Current request aborted
   - Can detect and handle
   - Other clients wait (queued)
   âš ï¸ Impact: Delays for queued requests

2. Disk I/O error:
   - Easy to catch and handle
   - Return error to client
   - Continue with next request
   âœ… Good recovery

3. Process crash:
   - ALL connections lost âŒ
   - No in-flight request recovery
   - Clients must retry
   âŒ Poor fault isolation

4. Resource exhaustion:
   - Memory leak â†’ OOM
   - Entire server dies
   âŒ No graceful degradation

Fault tolerance score: 5/10 âš ï¸
```

---

**F. TÃ³m táº¯t Single-threaded**

| TiÃªu chÃ­ | ÄÃ¡nh giÃ¡ | Äiá»ƒm | LÃ½ do |
|----------|----------|------|-------|
| **Performance** | âŒ Ráº¥t kÃ©m | 1/10 | 50 req/sec vs 10K required |
| **Scalability** | âŒ Ráº¥t kÃ©m | 1/10 | KhÃ´ng dÃ¹ng Ä‘Æ°á»£c multicore |
| **Development** | âœ… Tá»‘t | 9/10 | Code Ä‘Æ¡n giáº£n, dá»… debug |
| **Fault Tolerance** | âš ï¸ TB | 5/10 | No isolation, but predictable |
| **OVERALL** | âŒ | **4/10** | **KHÃ”NG phÃ¹ há»£p** |

---

#### **Pháº§n 3: MÃ´ hÃ¬nh 2 - Multi-threaded (Äa luá»“ng)**

**A. Kiáº¿n trÃºc Multi-threaded**

**Model 2a: Thread-per-request**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FILE SERVER PROCESS                    â”‚
â”‚                                                â”‚
â”‚  Main Thread:                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ while (1) {      â”‚                         â”‚
â”‚  â”‚   client = accept()  â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   spawn_thread(client) â”€â”€â”€â”¼â”€â”€â”€â”           â”‚
â”‚  â”‚ }                â”‚         â”‚   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚           â”‚
â”‚                                â”‚   â”‚           â”‚
â”‚  Worker Threads:               â”‚   â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Thread 1   â”‚  â”‚ Thread 2     â”‚  â”‚ Thread 3 â”‚â”‚
â”‚  â”‚ handle_req â”‚  â”‚ handle_req   â”‚  â”‚handle_reqâ”‚â”‚
â”‚  â”‚  (client1) â”‚  â”‚  (client2)   â”‚  â”‚(client3) â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚       ...            ...               ...      â”‚
â”‚  (Thousands of threads)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Code example:
c// Multi-threaded file server
void* worker_thread(void* arg) {
    int client_fd = *(int*)arg;
    free(arg);
    
    // Read request
    Request req;
    read(client_fd, &req, sizeof(req));
    
    // Process (this thread blocks on I/O, others continue!)
    Response resp = handle_file_request(&req);
    
    // Send response
    write(client_fd, &resp, sizeof(resp));
    
    close(client_fd);
    return NULL;
}

void multi_threaded_server() {
    int server_fd = socket(AF_INET, SOCK_STREAM, 0);
    bind(server_fd, ...);
    listen(server_fd, BACKLOG);
    
    while (1) {
        int client_fd = accept(server_fd, ...);
        
        // Spawn new thread for each request
        pthread_t thread;
        int* arg = malloc(sizeof(int));
        *arg = client_fd;
        
        pthread_create(&thread, NULL, worker_thread, arg);
        pthread_detach(thread);  // Auto-cleanup
        
        // Main thread immediately available for next accept!
    }
}
```

**Model 2b: Thread pool (Better!)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Thread:                                  â”‚
â”‚  accept() â†’ enqueue(request_queue)            â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚        Request Queue (bounded)           â”‚ â”‚
â”‚  â”‚  [R1] [R2] [R3] [R4] ... [RN]           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚     â”‚     â”‚     â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â” â”Œâ”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚Thread1â”‚ â”‚Thr 2â”‚ â”‚Thr3â”‚ â”‚ ... â”‚           â”‚
â”‚  â”‚Worker â”‚ â”‚Work â”‚ â”‚Workâ”‚ â”‚ThrN â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜           â”‚
â”‚    (Fixed pool size: e.g., 100 threads)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
c// Thread pool implementation
typedef struct {
    pthread_t threads[POOL_SIZE];
    Queue* request_queue;
    pthread_mutex_t queue_lock;
    pthread_cond_t queue_cond;
    int shutdown;
} ThreadPool;

void* pool_worker(void* arg) {
    ThreadPool* pool = (ThreadPool*)arg;
    
    while (!pool->shutdown) {
        pthread_mutex_lock(&pool->queue_lock);
        
        // Wait for work
        while (queue_empty(pool->request_queue) && !pool->shutdown) {
            pthread_cond_wait(&pool->queue_cond, &pool->queue_lock);
        }
        
        if (pool->shutdown) {
            pthread_mutex_unlock(&pool->queue_lock);
            break;
        }
        
        // Dequeue request
        Request* req = queue_dequeue(pool->request_queue);
        pthread_mutex_unlock(&pool->queue_lock);
        
        // Process (no lock held!)
        handle_file_request(req);
    }
    
    return NULL;
}

void thread_pool_server() {
    ThreadPool pool;
    init_thread_pool(&pool, POOL_SIZE);
    
    int server_fd = socket(...);
    listen(server_fd, BACKLOG);
    
    while (1) {
        int client_fd = accept(server_fd, ...);
        
        Request* req = malloc(sizeof(Request));
        req->client_fd = client_fd;
        
        // Enqueue to pool
        pthread_mutex_lock(&pool.queue_lock);
        queue_enqueue(pool.request_queue, req);
        pthread_cond_signal(&pool.queue_cond);
        pthread_mutex_unlock(&pool.queue_lock);
    }
}
```

---

**B. PhÃ¢n tÃ­ch hiá»‡u nÄƒng (Performance) - Ráº¤T Tá»T âœ…**

**1. Throughput - CAO âœ…**
```
Timeline with 100 threads:

Time â†’
0ms   10ms  20ms  30ms
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
Thread 1: [R1â”€â”€â”€â”€]
Thread 2:   [R2â”€â”€â”€â”€]
Thread 3:     [R3â”€â”€â”€â”€]
...
Thread 100:         [R100â”€â”€]

Concurrent processing: 100 requests
Each request: 10ms
Throughput: 100 / 0.01s = 10,000 req/sec âœ…

Matches requirement! âœ…
```

**2. CPU Utilization - Tá»T âœ…**
```
With 100 threads on 8-core CPU:

When thread blocks on I/O:
- OS schedules another thread
- Cores stay busy

CPU usage:
- Core 1: Thread1, Thread9, Thread17... (90% busy)
- Core 2: Thread2, Thread10, Thread18... (90% busy)
- ...
- Core 8: Thread8, Thread16, Thread24... (90% busy)

Total CPU utilization: ~90% âœ…
Context switches: ~10,000/sec (manageable)

Effectively uses multicore! âœ…
```

**3. Latency - Tá»T âœ…**
```
Request latency:

Low load (threads available):
- Queue time: ~0ms
- Processing: 10ms
- Total: 10ms âœ…

High load (all threads busy):
- Queue time: ~10ms (wait for thread)
- Processing: 10ms
- Total: 20ms
- P95: <20ms âœ… (still good!)

Meets requirement: P95 < 10ms with headroom
```

---

**C. Kháº£ nÄƒng má»Ÿ rá»™ng (Scalability) - Tá»T âœ…**
```
Vertical scaling (more cores):

1 core:  1,250 req/sec
2 cores: 2,500 req/sec (2x)
4 cores: 5,000 req/sec (4x)
8 cores: 10,000 req/sec (8x) âœ…

Linear scaling! âœ…

Horizontal scaling (more servers):

1 server:  10,000 req/sec
2 servers: 20,000 req/sec
4 servers: 40,000 req/sec

Also linear! âœ…

Tuning thread pool size:
- Too small (10 threads): Underutilized, 1,250 req/sec
- Optimal (100 threads): Balanced, 10,000 req/sec âœ…
- Too large (10,000 threads): Overhead, 9,000 req/sec âš ï¸

Formula: threads = cores Ã— (1 + wait_time/compute_time)
       = 8 Ã— (1 + 7ms/3ms) â‰ˆ 27 threads (minimum)
       = 100 threads (with headroom) âœ…

Scalability score: 9/10 âœ…

D. Äá»™ phá»©c táº¡p phÃ¡t triá»ƒn (Development Complexity) - TRUNG BÃŒNH âš ï¸
c// Challenges:

1. Synchronization required âš ï¸
pthread_mutex_t file_cache_lock;
pthread_rwlock_t metadata_lock;

// Must protect shared data structures
void update_metadata(File* file) {
    pthread_rwlock_wrlock(&metadata_lock);
    // Critical section
    file->last_modified = time(NULL);
    pthread_rwlock_unlock(&metadata_lock);
}

2. Race conditions possible âŒ
// Bug: Lost update
// Thread 1:                    Thread 2:
count = get_count();       count = get_count();  // Both read 10
count++;                   count++;              // Both compute 11
set_count(count);          set_count(count);     // Both write 11
// Expected: 12, Actual: 11 âŒ

3. Deadlocks possible âŒ
// Thread 1:                    Thread 2:
lock(mutex_A);             lock(mutex_B);
lock(mutex_B); // WAIT     lock(mutex_A); // WAIT
// DEADLOCK! âŒ

4. Memory leaks âš ï¸
void* worker(void* arg) {
    Request* req = (Request*)arg;
    handle_request(req);
    // Forgot to free(req)! âŒ
}

5. Thread lifecycle management
// Resource leak if not careful
pthread_t thread;
pthread_create(&thread, NULL, worker, data);
// Must: pthread_join() or pthread_detach()
// Otherwise: Thread resources not freed âŒ
```

**Development complexity:**
```
Code lines: ~1,500 (vs 200 for single-threaded)
Development time: 4-6 weeks (vs 1 week)
Bug density: Higher (race conditions, deadlocks)
Debugging: Harder (non-deterministic bugs)
Testing: More complex (need concurrency tests)

Complexity score: 6/10 âš ï¸
```

---

**E. Kháº£ nÄƒng chá»‹u lá»—i (Fault Tolerance) - Tá»T âœ…**
```
Failure isolation:

1. Thread crash:
   âœ… Only that request fails
   âœ… Other threads unaffected
   âœ… Server continues
   
   Example:
   Thread 42: Segfault in handle_request()
   â†’ Signal handler catches
   â†’ Thread terminated
   â†’ Threads 1-41, 43-100: Normal operation âœ…

2. Client disconnect:
   âœ… Thread detects (EPIPE)
   âœ… Cleans up resources
   âœ… Returns to pool
   âœ… No impact on other clients

3. Resource exhaustion:
   âš ï¸ Can be managed with bounded queues
   
   if (queue_full) {
       send_error(client, "503 Server Busy");
       // Graceful rejection âœ…
   }

4. Deadlock detection:
   âš ï¸ Possible with timeout mechanisms
   
   if (pthread_mutex_timedlock(&lock, &timeout) != 0) {
       // Timeout, possibly deadlock
       log_error("Potential deadlock");
       abort_request();
   }

Fault tolerance score: 8/10 âœ…

F. Tá»‘i Æ°u hÃ³a cho Read-heavy workload
c// Reader-Writer locks for read-heavy
pthread_rwlock_t cache_lock = PTHREAD_RWLOCK_INITIALIZER;

// Read path (concurrent, fast)
Response read_file(Request* req) {
    pthread_rwlock_rdlock(&cache_lock);  // Many readers OK!
    
    File* file = lookup_cache(req->filename);
    if (file) {
        Data data = file->data;
        pthread_rwlock_unlock(&cache_lock);
        return create_response(data);  // Fast! âœ…
    }
    
    pthread_rwlock_unlock(&cache_lock);
    
    // Cache miss: Read from disk
    file = read_from_disk(req->filename);
    
    pthread_rwlock_wrlock(&cache_lock);  // Exclusive for write
    insert_cache(file);
    pthread_rwlock_unlock(&cache_lock);
    
    return create_response(file->data);
}

// Write path (exclusive, infrequent)
Response write_file(Request* req) {
    pthread_rwlock_wrlock(&cache_lock);  // Exclusive lock
    
    write_to_disk(req->filename, req->data);
    update_cache(req->filename, req->data);
    
    pthread_rwlock_unlock(&cache_lock);
    
    return create_response(OK);
}

Performance:
- 95% reads: Nearly zero lock contention âœ…
- 5% writes: Occasional exclusive lock (acceptable)
- Overall: Excellent throughput âœ…
```

---

**G. TÃ³m táº¯t Multi-threaded**

| TiÃªu chÃ­ | ÄÃ¡nh giÃ¡ | Äiá»ƒm | LÃ½ do |
|----------|----------|------|-------|
| **Performance** | âœ… Ráº¥t tá»‘t | 9/10 | 10K req/sec, low latency |
| **Scalability** | âœ… Tá»‘t | 9/10 | Linear scaling, multicore |
| **Development** | âš ï¸ TB | 6/10 | Sync complexity, bugs |
| **Fault Tolerance** | âœ… Tá»‘t | 8/10 | Good isolation, recovery |
| **OVERALL** | âœ… | **8/10** | **Ráº¥t phÃ¹ há»£p** âœ… |

---

#### **Pháº§n 4: MÃ´ hÃ¬nh 3 - Event-driven FSM (Non-blocking I/O)**

**A. Kiáº¿n trÃºc Event-driven**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FILE SERVER PROCESS                    â”‚
â”‚                                                â”‚
â”‚  Main Event Loop (single thread):             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ while (1) {                              â”‚ â”‚
â”‚  â”‚   events = epoll_wait(epoll_fd, ...);   â”‚ â”‚
â”‚  â”‚                                          â”‚ â”‚
â”‚  â”‚   for each event:                       â”‚ â”‚
â”‚  â”‚     if (event.type == ACCEPT)           â”‚ â”‚
â”‚  â”‚       handle_new_connection()           â”‚ â”‚
â”‚  â”‚     else if (event.type == READ)        â”‚ â”‚
â”‚  â”‚       handle_read_ready()               â”‚ â”‚
â”‚  â”‚     else if (event.type == WRITE)       â”‚ â”‚
â”‚  â”‚       handle_write_ready()              â”‚ â”‚
â”‚  â”‚     else if (event.type == DISK_IO)     â”‚ â”‚
â”‚  â”‚       handle_io_completion()            â”‚ â”‚
â”‚  â”‚ }                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Async I/O Thread Pool (optional)       â”‚ â”‚
â”‚  â”‚   [Worker 1] [Worker 2] ... [Worker N]  â”‚ â”‚
â”‚  â”‚   (For disk I/O only)                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key: Non-blocking I/O + Event multiplexing (epoll/kqueue)
Code example:
c// State machine for each connection
typedef enum {
    STATE_ACCEPTING,
    STATE_READING_REQUEST,
    STATE_PROCESSING,
    STATE_WRITING_RESPONSE,
    STATE_CLOSING
} ConnectionState;

typedef struct {
    int fd;
    ConnectionState state;
    Request req;
    Response resp;
    int bytes_read;
    int bytes_written;
} Connection;

// Main event loop
void event_driven_server() {
    int server_fd = socket(AF_INET, SOCK_STREAM, 0);
    set_nonblocking(server_fd);
    bind(server_fd, ...);
    listen(server_fd, BACKLOG);
    
    // Create epoll instance
    int epoll_fd = epoll_create1(0);
    
    // Add server socket to epoll
    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = server_fd;
    epoll_ctl(epoll_fd, EPOLL_CTL_ADD, server_fd, &ev);
    
    Connection* connections[MAX_CONNECTIONS];
    
    // Main event loop
    while (1) {
        struct epoll_event events[MAX_EVENTS];
        int nfds = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
        
        for (int i = 0; i < nfds; i++) {
            if (events[i].data.fd == server_fd) {
                // New connection
                handle_accept(epoll_fd, server_fd, connections);
            } else {
                // Existing connection has event
                Connection* conn = events[i].data.ptr;
                handle_connection_event(epoll_fd, conn, &events[i]);
            }
        }
    }
}

void handle_connection_event(int epoll_fd, Connection* conn, 
                             struct epoll_event* event) {
    switch (conn->state) {
        case STATE_READING_REQUEST:
            if (event->events & EPOLLIN) {
                // Socket readable
                int n = read(conn->fd, &conn->req + conn->bytes_read,
                           sizeof(Request) - conn->bytes_read);
                
                if (n > 0) {
                    conn->bytes_read += n;
                    
                    if (conn->bytes_read == sizeof(Request)) {
                        // Request complete
                        conn->state = STATE_PROCESSING;
                        
                        // Submit async I/O
                        submit_async_read(conn);
                    }
                } else if (n == 0) {
                    // EOF
                    close_connection(epoll_fd, conn);
                } else if (errno != EAGAIN) {
                    // Error
                    close_connection(epoll_fd, conn);
                }
            }
            break;
            
        case STATE_PROCESSING:
            // Async I/O completed (event from io_uring/AIO)
            if (event->events & EPOLLOUT) {
                conn->state = STATE_WRITING_RESPONSE;
                // Fall through to write
            }
            break;
            
        case STATE_WRITING_RESPONSE:
            if (event->events & EPOLLOUT) {
                // Socket writable
                int n = write(conn->fd, &conn->resp + conn->bytes_written,
                            sizeof(Response) - conn->bytes_written);
                
                if (n > 0) {
                    conn->bytes_written += n;
                    
                    if (conn->bytes_written == sizeof(Response)) {
                        // Response complete
                        conn->state = STATE_CLOSING;
                        close_connection(epoll_fd, conn);
                    }
                } else if (errno != EAGAIN) {
                    close_connection(epoll_fd, conn);
                }
            }
            break;
    }
}

// Async disk I/O (using io_uring or thread pool)
void submit_async_read(Connection* conn) {
    // Option 1: io_uring (Linux 5.1+)
    struct io_uring_sqe* sqe = io_uring_get_sqe(&ring);
    io_uring_prep_read(sqe, file_fd, buffer, size, offset);
    io_uring_sqe_set_data(sqe, conn);
    io_uring_submit(&ring);
    
    // Option 2: Thread pool for disk I/O
    // submit_to_io_thread_pool(conn);
}
```

---

**B. PhÃ¢n tÃ­ch hiá»‡u nÄƒng (Performance) - Ráº¤T Tá»T âœ…**

**1. Throughput - Ráº¤T CAO âœ…**
```
Event loop performance:

Single thread handles ALL network I/O:
- epoll_wait: O(1) for ready events
- No context switches for network I/O
- Process multiple events per iteration

Timeline:
Time â†’
0Î¼s   100Î¼s  200Î¼s  300Î¼s  400Î¼s
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
Event loop:
[R1 recv][R2 recv][R3 recv][R4 recv]...[R100 recv]
  â†“       â†“       â†“       â†“              â†“
[I/O]   [I/O]   [I/O]   [I/O]  ...    [I/O]
  â†“       â†“       â†“       â†“              â†“
[R1 send][R2 send][R3 send][R4 send]...[R100 send]

Process 100 requests in 400Î¼s batch
Throughput: 100 / 0.4ms = 250,000 req/sec âœ…âœ…âœ…

With async I/O thread pool (for disk):
Actual throughput: 50,000 req/sec
(Still 5x better than multi-threaded!)
```

**2. CPU Utilization - Tá»T âœ…**
```
CPU breakdown:

Main event thread:
- Network I/O handling: 30%
- Event processing: 20%
- Context management: 10%
- Total: 60% busy

Async I/O threads (4 threads):
- Disk I/O wait: 70%
- CPU: 30%

Overall:
- 1 main thread at 60% = 0.6 core
- 4 I/O threads at 30% = 1.2 cores
- Total: 1.8 cores used (out of 8)

Why so low?
- I/O bound workload (disk wait)
- Network I/O is async (no CPU during wait)

But: Can handle 50K req/sec with <2 cores! âœ…
Efficiency: 25,000 req/sec per core âœ…
(vs multi-threaded: 1,250 req/sec per core)

20x more efficient! âœ…âœ…âœ…
```

**3. Latency - THáº¤P âœ…**
```
Request latency breakdown:

Event-driven:
- Queue in event loop: <1ms
- Disk I/O (async): 5ms (concurrent)
- Network send: <1ms
- Total: ~7ms âœ…

Multi-threaded (comparison):
- Queue wait: 1-5ms (depends on thread availability)
- Disk I/O: 5ms
- Network send: 1ms
- Total: ~10ms

Event-driven: 30% lower latency âœ…

P95 latency:
- Event-driven: 8ms âœ…
- Multi-threaded: 15ms
```

---

**C. Kháº£ nÄƒng má»Ÿ rá»™ng (Scalability) - Ráº¤T Tá»T âœ…**
```
C10K problem (10,000 concurrent connections):

Multi-threaded:
- 10,000 threads
- Memory: 10K Ã— 8MB stack = 80 GB âŒ
- Context switches: Excessive
- Infeasible âŒ

Event-driven:
- 1 event loop thread
- 10,000 connections = 10K Ã— 4KB state = 40 MB âœ…
- No context switches for network I/O
- Feasible! âœ…

C100K (100,000 connections):
- Memory: 400 MB (acceptable)
- Performance: Minimal degradation
- Event-driven wins! âœ…

C1M (1 million connections):
- Memory: 4 GB (still OK on modern servers)
- Possible with event-driven âœ…
- Impossible with threads âŒ

Scalability score: 10/10 âœ…âœ…âœ…
```

**Horizontal scaling:**
```
Event-driven server:

1 server: 50,000 req/sec (using 2 cores)
2 servers: 100,000 req/sec
4 servers: 200,000 req/sec

Linear scaling âœ…

Cost efficiency:
- Can handle same load with fewer servers
- Lower infrastructure cost âœ…

D. Äá»™ phá»©c táº¡p phÃ¡t triá»ƒn (Development Complexity) - CAO âŒ
c// Challenge 1: Callback hell / State machine complexity

void handle_request(Connection* conn) {
    // Step 1: Read request
    read_async(conn->fd, &conn->req, sizeof(Request),
               on_request_read_complete, conn);
}

void on_request_read_complete(Connection* conn, int result) {
    if (result < 0) {
        handle_error(conn);
        return;
    }
    
    // Step 2: Read file
    read_file_async(conn->req.filename, conn->buffer, SIZE,
                   on_file_read_complete, conn);
}

void on_file_read_complete(Connection* conn, int result) {
    if (result < 0) {
        handle_error(conn);
        return;
    }
    
    // Step 3: Send response
    write_async(conn->fd, conn->buffer, result,
               on_response_write_complete, conn);
}

void on_response_write_complete(Connection* conn, int result) {
    // Step 4: Close
    close_connection(conn);
}

// â†’ Deeply nested callbacks, hard to follow âŒ
// â†’ Error handling scattered everywhere âŒ
// â†’ Debugging nightmare âŒ
Challenge 2: Shared state management
c// All state must be explicit in Connection struct
typedef struct {
    int fd;
    ConnectionState state;          // Explicit state
    Request req;
    Response resp;
    char buffer[BUFFER_SIZE];
    int bytes_read;                 // Partial I/O tracking
    int bytes_written;
    Timer* timeout;                 // Timeout handling
    void* user_data;                // Additional context
    // ... many more fields ...
} Connection;

// VS multi-threaded (implicit stack):
void handle_request(int client_fd) {
    Request req;               // On stack (automatic)
    read(client_fd, &req, ...); // Blocking, simple
    
    Response resp = process(req);
    write(client_fd, &resp, ...);
    // Stack automatically cleaned up
}

// Event-driven: Manual memory management âŒ
// Must allocate, track, and free Connection âš ï¸
Challenge 3: Error handling
c// Every callback must handle errors
void on_read(Connection* conn, int result) {
    if (result < 0) {
        if (errno == EAGAIN) {
            // Retry later (re-arm event)
            return;
        } else if (errno == ECONNRESET) {
            // Client disconnect
            cleanup_connection(conn);
            return;
        } else {
            // Other error
            log_error(errno);
            cleanup_connection(conn);
            return;
        }
    }
    
    // Normal path
    // ...
}

// Error handling interleaved with logic âŒ
// Easy to miss edge cases âŒ
```

**Development complexity:**
```
Code lines: ~3,000 (vs 1,500 multi-threaded)
Development time: 8-12 weeks âŒ
Learning curve: Steep (event-driven paradigm)
Debugging: Very difficult (async, state machines)
Testing: Complex (must test all state transitions)

Common bugs:
- State machine errors
- Memory leaks (forgot to free Connection)
- Use-after-free (callback on closed connection)
- Race conditions (even with single thread!)

Complexity score: 3/10 âŒ
```

---

**E. Kháº£ nÄƒng chá»‹u lá»—i (Fault Tolerance) - TRUNG BÃŒNH âš ï¸**
```
Pros:

1. Single-threaded main loop:
   âœ… No race conditions on event processing
   âœ… Deterministic behavior
   âœ… Easier to reason about

2. Graceful handling of slow clients:
   âœ… Non-blocking I/O â†’ No blocking on slow write
   âœ… Can handle millions of connections
   âœ… Backpressure naturally handled

Cons:

1. Single point of failure:
   âŒ Main event loop crashes â†’ Entire server dies
   âŒ Bug in event handler â†’ All requests affected
   
   Example:
   void buggy_handler(Connection* conn) {
       int* ptr = NULL;
       *ptr = 42;  // Segfault!
       // â†’ Entire server crashes âŒ
   }

2. No isolation:
   âŒ CPU-intensive operation blocks event loop
   
   void expensive_operation() {
       for (int i = 0; i < 1000000000; i++) {
           compute();  // 10 seconds!
       }
       // â†’ ALL connections frozen âŒ
   }

3. Partial failure complexity:
   âš ï¸ Connection in middle of state machine
   âš ï¸ Must handle cleanup carefully
   
   if (conn->state == STATE_READING_REQUEST) {
       // Partial read, must cleanup buffer
       free(conn->req.partial_data);
   }

4. Cascade failures:
   âš ï¸ Slow disk I/O â†’ I/O thread pool exhausted
   âš ï¸ â†’ All requests queue up
   âš ï¸ â†’ Memory pressure
   âš ï¸ â†’ OOM

Fault tolerance score: 6/10 âš ï¸

F. Optimization for Read-heavy workload
c// Event-driven is PERFECT for read-heavy!

// Cache in main thread (no locking needed!)
HashMap* file_cache;  // Single-threaded, no locks âœ…

void handle_read_request(Connection* conn) {
    // Check cache (O(1), no lock)
    File* file = hashmap_get(file_cache, conn->req.filename);
    
    if (file) {
        // Cache hit: Serve immediately âœ…
        conn->resp.data = file->data;
        conn->state = STATE_WRITING_RESPONSE;
        
        // No disk I/O needed!
        // Latency: <1ms âœ…âœ…âœ…
    } else {
        // Cache miss: Async disk read
        submit_async_read(conn);
        // Other requests continue processing âœ…
    }
}

// Write path: Rare, so can be slower
void handle_write_request(Connection* conn) {
    // Invalidate cache
    hashmap_remove(file_cache, conn->req.filename);
    
    // Submit async write
    submit_async_write(conn);
}

Performance for 95% reads:
- 90% cache hits: <1ms latency âœ…âœ…âœ…
- 5% cache misses: 7ms latency (async I/O)
- 5% writes: 20ms latency (acceptable)

Average latency: 0.9Ã—1ms + 0.05Ã—7ms + 0.05Ã—20ms
              = 0.9 + 0.35 + 1
              = 2.25ms âœ…âœ…âœ…

MUCH better than multi-threaded (10ms) âœ…

G. TÃ³m táº¯t Event-driven FSM
TiÃªu chÃ­ÄÃ¡nh giÃ¡Äiá»ƒmLÃ½ doPerformanceâœ…âœ… Xuáº¥t sáº¯c10/1050K req/sec, 2ms latencyScalabilityâœ…âœ… Xuáº¥t sáº¯c10/10C1M possible, efficientDevelopmentâŒ KhÃ³3/10Complex, callback hellFault Toleranceâš ï¸ TB6/10SPOF, but deterministicOVERALLâœ…7.25/10PhÃ¹ há»£p náº¿u cÃ³ expertise
Pháº§n 5: So sÃ¡nh tá»•ng há»£p vÃ  Khuyáº¿n nghá»‹
A. Báº£ng so sÃ¡nh tá»•ng quan
TiÃªu chÃ­Single-threadedMulti-threadedEvent-driven FSMTrá»ng sá»‘Throughput50 req/s âŒ10,000 req/s âœ…50,000 req/s âœ…âœ…30%Latency (P95)210ms âŒ15ms âœ…8ms âœ…âœ…25%CPU Efficiency10% âŒ90% âœ…60% âœ…10%Memory per connN/A8 MB âŒ4 KB âœ…âœ…10%Max connections100 âŒ1,000 âš ï¸1,000,000 âœ…âœ…15%Dev complexitySimple âœ…âœ…Medium âš ï¸Complex âŒ5%Fault isolationNone âŒGood âœ…Medium âš ï¸5%WEIGHTED SCORE1.8/108.3/10 âœ…9.1/10 âœ…âœ…-

B. Chi tiáº¿t phÃ¢n tÃ­ch tá»«ng tiÃªu chÃ­
1. Hiá»‡u nÄƒng (Performance) - Trá»ng sá»‘ 55%
Throughput:
Benchmark: Sustained load test

Single-threaded:
â”œâ”€ Requests/sec: 50
â”œâ”€ Bottleneck: Sequential processing
â””â”€ Rating: 1/10 âŒ

Multi-threaded (100 threads):
â”œâ”€ Requests/sec: 10,000
â”œâ”€ Bottleneck: Context switch overhead
â”œâ”€ Rating: 8/10 âœ…

Event-driven:
â”œâ”€ Requests/sec: 50,000
â”œâ”€ Bottleneck: Disk I/O (with io_uring)
â””â”€ Rating: 10/10 âœ…âœ…

Winner: Event-driven (5x better than multi-threaded)
Latency:
Latency distribution (milliseconds):

                P50   P95   P99   P99.9
Single-thread:   20   210   500   1000   âŒ
Multi-thread:    10    15    25     50   âœ…
Event-driven:     2     8    12     20   âœ…âœ…

Latency variance:
Single-thread: High (depends on queue)
Multi-thread:  Medium (thread scheduling)
Event-driven:  Low (predictable event loop) âœ…

Winner: Event-driven
CPU Efficiency:
CPU efficiency = Useful work / CPU cycles

Single-threaded:
- CPU usage: 10%
- Efficiency: 50 req/s per 10% = 5 req/s per 1% CPU
- Rating: 1/10

Multi-threaded:
- CPU usage: 90% (8 cores)
- Efficiency: 10K req/s per 720% = 13.9 req/s per 1% CPU
- Rating: 8/10

Event-driven:
- CPU usage: 60% (1.8 cores = 22.5%)
- Efficiency: 50K req/s per 22.5% = 2,222 req/s per 1% CPU âœ…âœ…
- Rating: 10/10

Winner: Event-driven (160x better than multi-threaded!)
Memory Efficiency:
Memory per connection:

Single-threaded: ~0 (sequential, no state)
                but only handles 1 at a time âŒ

Multi-threaded:
â”œâ”€ Stack per thread: 8 MB
â”œâ”€ 1,000 connections = 8 GB
â”œâ”€ Practical limit: ~1,000 concurrent
â””â”€ Rating: 4/10 âš ï¸

Event-driven:
â”œâ”€ State per connection: 4 KB
â”œâ”€ 100,000 connections = 400 MB âœ…
â”œâ”€ Practical limit: 1,000,000+ concurrent
â””â”€ Rating: 10/10 âœ…âœ…

Winner: Event-driven (2,000x better!)

2. Kháº£ nÄƒng má»Ÿ rá»™ng (Scalability) - Trá»ng sá»‘ 15%
Vertical Scaling (thÃªm cores):
8-core CPU â†’ 16-core CPU

Single-threaded:
â”œâ”€ 50 req/s â†’ 50 req/s (NO CHANGE âŒ)
â”œâ”€ Uses only 1 core
â””â”€ Scaling factor: 0x

Multi-threaded:
â”œâ”€ 10K req/s â†’ 20K req/s âœ…
â”œâ”€ Linear scaling
â””â”€ Scaling factor: 2x âœ…

Event-driven:
â”œâ”€ 50K req/s â†’ 80K req/s âœ…
â”œâ”€ Near-linear (I/O bound)
â””â”€ Scaling factor: 1.6x âœ…

Winner: Multi-threaded (perfectly linear)
Horizontal Scaling (thÃªm servers):
All scale linearly:
â”œâ”€ Single-threaded: 50 â†’ 100 req/s (2 servers)
â”œâ”€ Multi-threaded:  10K â†’ 20K req/s
â””â”€ Event-driven:    50K â†’ 100K req/s âœ…

But event-driven needs fewer servers:
Target: 100K req/s

Single-threaded: 2,000 servers! âŒ ($$$$$)
Multi-threaded:  10 servers âœ… ($$$)
Event-driven:    2 servers âœ…âœ… ($)

Winner: Event-driven (cost efficiency)
Connection Scalability:
Maximum concurrent connections:

Single-threaded:
â”œâ”€ Sequential â†’ Effectively 1 connection
â””â”€ Rating: 1/10 âŒ

Multi-threaded:
â”œâ”€ Limit: ~10,000 threads (OS/memory limit)
â”œâ”€ Real-world: ~1,000 threads (performance)
â””â”€ Rating: 6/10 âš ï¸

Event-driven:
â”œâ”€ Limit: ~10,000,000 (fd limit)
â”œâ”€ Real-world: 1,000,000+ (C1M problem solved!)
â””â”€ Rating: 10/10 âœ…âœ…

Winner: Event-driven (1,000x better!)

3. Äá»™ phá»©c táº¡p phÃ¡t triá»ƒn (Development Complexity) - Trá»ng sá»‘ 5%
Code Complexity:
Lines of Code:

Single-threaded: 200 LOC
â”œâ”€ Simple sequential flow
â”œâ”€ Easy to understand
â””â”€ Rating: 10/10 âœ…âœ…

Multi-threaded: 1,500 LOC
â”œâ”€ Synchronization code: 400 LOC
â”œâ”€ Thread pool management: 300 LOC
â”œâ”€ Error handling: 200 LOC
â””â”€ Rating: 6/10 âš ï¸

Event-driven: 3,000 LOC
â”œâ”€ State machine: 800 LOC
â”œâ”€ Callback management: 700 LOC
â”œâ”€ Async I/O: 600 LOC
â”œâ”€ Error handling everywhere: 400 LOC
â””â”€ Rating: 3/10 âŒ

Winner: Single-threaded (but unusable for production)
Development Time:
Team: 2 mid-level engineers

Single-threaded:
â”œâ”€ Implementation: 1 week
â”œâ”€ Testing: 1 week
â”œâ”€ Total: 2 weeks âœ…
â””â”€ But: Cannot meet requirements âŒ

Multi-threaded:
â”œâ”€ Implementation: 4 weeks
â”œâ”€ Thread-safety testing: 2 weeks
â”œâ”€ Bug fixing (race conditions): 2 weeks
â”œâ”€ Total: 8 weeks âœ…
â””â”€ Rating: 7/10

Event-driven:
â”œâ”€ Learning curve: 2 weeks
â”œâ”€ Implementation: 6 weeks
â”œâ”€ State machine testing: 3 weeks
â”œâ”€ Debugging async issues: 3 weeks
â”œâ”€ Total: 14 weeks âš ï¸
â””â”€ Rating: 4/10

Winner: Multi-threaded (balanced)
Debugging Difficulty:
Bug: "Server sometimes returns wrong file"

Single-threaded:
â”œâ”€ Add printf, reproduce easily
â”œâ”€ gdb step-through works
â”œâ”€ Time to fix: 1 hour âœ…
â””â”€ Rating: 10/10

Multi-threaded:
â”œâ”€ Race condition: Hard to reproduce
â”œâ”€ Use thread sanitizer (TSan)
â”œâ”€ Helgrind, valgrind
â”œâ”€ Time to fix: 1 day âš ï¸
â””â”€ Rating: 5/10

Event-driven:
â”œâ”€ State machine bug: Hard to reproduce
â”œâ”€ Async callback timing issue
â”œâ”€ Need to add extensive logging
â”œâ”€ Reconstruct state from logs
â”œâ”€ Time to fix: 3 days âŒ
â””â”€ Rating: 3/10

Winner: Single-threaded (but impractical)
Practical winner: Multi-threaded
Maintainability:
Code maintainability (1 year later):

Single-threaded:
â””â”€ Easy to modify âœ… (but still slow)

Multi-threaded:
â”œâ”€ Synchronization complexity
â”œâ”€ Need to understand locking strategy
â”œâ”€ Potential to introduce deadlocks
â””â”€ Moderate difficulty âš ï¸

Event-driven:
â”œâ”€ State machine interdependencies
â”œâ”€ Callback chains hard to follow
â”œâ”€ Adding new state is risky
â””â”€ High difficulty âŒ

Winner: Multi-threaded (best balance)

4. Kháº£ nÄƒng chá»‹u lá»—i (Fault Tolerance) - Trá»ng sá»‘ 5%
Failure Isolation:
Scenario: One request causes segfault

Single-threaded:
â”œâ”€ Entire process crashes âŒ
â”œâ”€ ALL clients disconnected
â”œâ”€ Requires process restart
â””â”€ Impact: 100% of users âŒ

Multi-threaded:
â”œâ”€ Only faulting thread crashes âœ…
â”œâ”€ Other threads continue
â”œâ”€ 99 out of 100 requests succeed
â””â”€ Impact: 1% of users âœ…

Event-driven:
â”œâ”€ Main loop crashes âŒ
â”œâ”€ ALL clients disconnected
â”œâ”€ (Unless segfault in I/O thread)
â””â”€ Impact: 100% or 0% âš ï¸

Winner: Multi-threaded
Graceful Degradation:
Scenario: High load (2x normal)

Single-threaded:
â”œâ”€ Queue grows indefinitely
â”œâ”€ Latency increases linearly
â”œâ”€ Eventually OOM âŒ
â””â”€ Rating: 2/10

Multi-threaded:
â”œâ”€ Bounded thread pool
â”œâ”€ Request queue with limit
â”œâ”€ Reject with 503 when full âœ…
â”œâ”€ Existing requests still processed
â””â”€ Rating: 8/10 âœ…

Event-driven:
â”œâ”€ Can accept many connections
â”œâ”€ But I/O thread pool saturated
â”œâ”€ All requests slow âš ï¸
â”œâ”€ Need backpressure mechanism
â””â”€ Rating: 6/10 âš ï¸

Winner: Multi-threaded
Recovery from Failures:
Client disconnect mid-request:

Single-threaded:
â”œâ”€ Detect on next I/O
â”œâ”€ Abort current request
â”œâ”€ Continue with next
â””â”€ Clean recovery âœ…

Multi-threaded:
â”œâ”€ Thread detects EPIPE
â”œâ”€ Cleanup resources
â”œâ”€ Thread returns to pool
â””â”€ Clean recovery âœ…

Event-driven:
â”œâ”€ Event: EPOLLHUP
â”œâ”€ Must cleanup state machine
â”œâ”€ Free allocated memory
â”œâ”€ More complex but works âœ…
â””â”€ Clean recovery âœ…

Winner: Tie (all handle well)
Monitoring and Observability:
Health check and metrics:

Single-threaded:
â”œâ”€ Simple state
â”œâ”€ Easy to monitor
â”œâ”€ Metrics: Current request
â””â”€ Rating: 8/10

Multi-threaded:
â”œâ”€ Per-thread metrics
â”œâ”€ Lock contention stats
â”œâ”€ Thread pool utilization
â””â”€ Rating: 9/10 âœ…

Event-driven:
â”œâ”€ Event loop latency
â”œâ”€ Connection state distribution
â”œâ”€ Async I/O queue depth
â”œâ”€ More metrics needed
â””â”€ Rating: 7/10

Winner: Multi-threaded

C. PhÃ¢n tÃ­ch theo workload characteristics
Read-heavy workload (95% reads):
Multi-threaded advantages:
âœ… Reader-writer locks allow concurrent reads
âœ… Minimal lock contention
âœ… Simple to implement

Code:
pthread_rwlock_t cache_lock;

// 95% of requests (CONCURRENT)
pthread_rwlock_rdlock(&cache_lock);
data = cache_get(key);
pthread_rwlock_unlock(&cache_lock);

// 5% of requests (EXCLUSIVE)
pthread_rwlock_wrlock(&cache_lock);
cache_put(key, data);
pthread_rwlock_unlock(&cache_lock);

Performance: Excellent âœ…
Event-driven advantages:
âœ…âœ… NO LOCKS needed (single-threaded cache)
âœ…âœ… Zero contention
âœ…âœ… Maximum throughput

Code:
// Global cache (no locks!)
HashMap cache;

// 95% reads
data = hashmap_get(&cache, key);  // O(1), no lock âœ…

// 5% writes
hashmap_put(&cache, key, data);   // O(1), no lock âœ…

Performance: Outstanding âœ…âœ…
Latency: <1ms for cache hits âœ…âœ…
Winner for read-heavy: Event-driven âœ…âœ…

Write-light workload (5% writes):
Multi-threaded:
â”œâ”€ Writer lock acquired infrequently (5%)
â”œâ”€ Minimal blocking of readers
â”œâ”€ Acceptable overhead
â””â”€ Rating: 8/10 âœ…

Event-driven:
â”œâ”€ No lock overhead ever
â”œâ”€ Writes go to async I/O queue
â”œâ”€ Readers continue unaffected
â””â”€ Rating: 10/10 âœ…âœ…

Winner: Event-driven

I/O-bound workload (file service):
Characteristics:
- 70% time in disk I/O
- 20% time in network I/O
- 10% time in CPU processing

Multi-threaded:
â”œâ”€ Threads block on I/O
â”œâ”€ OS schedules other threads
â”œâ”€ CPU utilized during I/O wait âœ…
â””â”€ Rating: 8/10

Event-driven:
â”œâ”€ Non-blocking I/O
â”œâ”€ Async I/O with io_uring
â”œâ”€ CPU processes other requests during I/O âœ…âœ…
â”œâ”€ Zero context switch overhead
â””â”€ Rating: 10/10 âœ…âœ…

Winner: Event-driven

D. Trade-offs Analysis
Multi-threaded trade-offs:
PROS:
âœ… Good performance (10K req/s)
âœ… Simple mental model (sequential per thread)
âœ… Uses multicore effectively
âœ… Good fault isolation
âœ… Moderate development complexity
âœ… Standard paradigm (many developers know)
âœ… Good debugging tools (gdb, TSan)

CONS:
âŒ Context switch overhead (~5Î¼s per switch)
âŒ Memory per connection (8 MB)
âŒ Limited connections (~1,000 practical)
âŒ Lock contention possible
âŒ Race conditions possible
âŒ Deadlocks possible

BEST FOR:
âœ“ Teams with thread programming experience
âœ“ Requirements: 1K-10K concurrent connections
âœ“ Mixed workloads (not purely I/O bound)
âœ“ Need quick time-to-market
âœ“ Standard enterprise applications
Event-driven trade-offs:
PROS:
âœ…âœ… Excellent performance (50K req/s)
âœ…âœ… Ultra-low memory (4 KB per conn)
âœ…âœ… Massive connections (1M+)
âœ…âœ… No lock overhead for single-threaded cache
âœ…âœ… No context switch for network I/O
âœ… Deterministic behavior
âœ… Cache-friendly (single thread)

CONS:
âŒâŒ High development complexity
âŒâŒ Callback hell / state machine complexity
âŒâŒ Difficult debugging
âŒâŒ Steep learning curve
âŒ Single point of failure
âŒ Must be careful with blocking operations
âŒ Fewer developers with expertise

BEST FOR:
âœ“ High-performance requirements (>20K req/s)
âœ“ Massive concurrency (>10K connections)
âœ“ I/O-bound workloads
âœ“ Read-heavy workloads
âœ“ Expert team with async programming experience
âœ“ Long-term project (amortize development cost)

E. Khuyáº¿n nghá»‹ cho File Service (Read-heavy)
PhÃ¢n tÃ­ch cá»¥ thá»ƒ:
File Service Requirements:
â”œâ”€ Workload: 95% reads, 5% writes âœ… Read-heavy
â”œâ”€ Expected load: 10K+ req/s
â”œâ”€ Latency requirement: P95 < 10ms
â”œâ”€ Concurrent connections: 10K-100K
â””â”€ Development resources: Medium-sized team
KHUYáº¾N NGHá»Š CHÃNH:
ğŸ† CHá»ŒN MULTI-THREADED (Thread Pool)
LÃ½ do:
1. PERFORMANCE: ÄÃ¡p á»©ng yÃªu cáº§u âœ…
   â”œâ”€ 10K req/s: Achievable
   â”œâ”€ P95 < 10ms: Achievable
   â”œâ”€ Reader-writer locks: Minimal contention
   â””â”€ Can optimize for 50K req/s if needed

2. SCALABILITY: Äá»§ tá»‘t âœ…
   â”œâ”€ 10K connections: Comfortable
   â”œâ”€ 100K connections: Possible with tuning
   â”œâ”€ Linear vertical scaling
   â””â”€ Linear horizontal scaling

3. DEVELOPMENT: Reasonable âœ…âœ…
   â”œâ”€ 8 weeks development time
   â”œâ”€ Standard paradigm
   â”œâ”€ Good tooling support
   â”œâ”€ Easier to hire developers
   â””â”€ Lower risk

4. MAINTENANCE: Manageable âœ…
   â”œâ”€ Moderate complexity
   â”œâ”€ Standard debugging tools
   â”œâ”€ Well-understood patterns
   â””â”€ Good documentation available

5. FAULT TOLERANCE: Good âœ…
   â”œâ”€ Thread isolation
   â”œâ”€ Graceful degradation
   â””â”€ Easy monitoring

Kiáº¿n trÃºc Ä‘á» xuáº¥t (Multi-threaded):
c// Recommended architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FILE SERVER (Multi-threaded)       â”‚
â”‚                                             â”‚
â”‚  Main Thread:                               â”‚
â”‚  â”œâ”€ Accept connections                      â”‚
â”‚  â””â”€ Enqueue to request queue               â”‚
â”‚                                             â”‚
â”‚  Worker Thread Pool (100 threads):          â”‚
â”‚  â”œâ”€ Reader threads (80): Handle reads       â”‚
â”‚  â”‚   â””â”€ pthread_rwlock_rdlock()            â”‚
â”‚  â”œâ”€ Writer threads (20): Handle writes      â”‚
â”‚  â”‚   â””â”€ pthread_rwlock_wrlock()            â”‚
â”‚  â””â”€ Async I/O helpers (optional)           â”‚
â”‚                                             â”‚
â”‚  Shared Data Structures:                    â”‚
â”‚  â”œâ”€ File metadata cache (RW-locked)        â”‚
â”‚  â”œâ”€ LRU cache (RW-locked)                  â”‚
â”‚  â””â”€ Request queue (mutex-protected)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Optimizations:
1. Reader-writer locks for cache
2. Lock-free queue (if needed)
3. Per-core caching (advanced)
4. Async I/O with io_uring (optional)
c// Implementation sketch
typedef struct {
    pthread_rwlock_t lock;
    HashMap* metadata_cache;
    HashMap* data_cache;
    LRUCache* lru;
} SharedState;

void* reader_thread(void* arg) {
    while (1) {
        Request* req = dequeue();
        
        // Read path (shared lock)
        pthread_rwlock_rdlock(&state->lock);
        
        File* file = cache_get(state->data_cache, req->filename);
        
        pthread_rwlock_unlock(&state->lock);
        
        if (!file) {
            // Cache miss: read from disk
            file = read_from_disk(req->filename);
            
            // Update cache (exclusive lock)
            pthread_rwlock_wrlock(&state->lock);
            cache_put(state->data_cache, req->filename, file);
            pthread_rwlock_unlock(&state->lock);
        }
        
        send_response(req->client_fd, file->data);
    }
}

void* writer_thread(void* arg) {
    while (1) {
        Request* req = dequeue();
        
        // Write path (exclusive lock)
        pthread_rwlock_wrlock(&state->lock);
        
        cache_invalidate(state->data_cache, req->filename);
        write_to_disk(req->filename, req->data);
        
        pthread_rwlock_unlock(&state->lock);
        
        send_response(req->client_fd, OK);
    }
}
```

**Performance estimate:**
```
Configuration:
â”œâ”€ Thread pool: 100 threads (80 readers, 20 writers)
â”œâ”€ 8-core CPU
â”œâ”€ Cache hit rate: 90%

Expected performance:
â”œâ”€ Throughput: 12,000 req/s âœ…
â”‚   â”œâ”€ 90% cache hits: <1ms (10,800 req/s)
â”‚   â””â”€ 10% disk reads: 10ms (1,200 req/s)
â”œâ”€ Latency P95: 8ms âœ…
â”œâ”€ Max connections: 10,000 âœ…
â””â”€ CPU utilization: 85% âœ…

Meets all requirements! âœ…âœ…âœ…
```

---

**KHI NÃ€O NÃŠN CHá»ŒN EVENT-DRIVEN:**
```
Choose event-driven if:

1. SCALE requirements:
   âœ“ Need >50K req/s single server
   âœ“ Need >100K concurrent connections
   âœ“ Infrastructure cost is critical

2. TEAM capabilities:
   âœ“ Have expert async programmers
   âœ“ Long development timeline acceptable
   âœ“ Complex debugging not a concern

3. WORKLOAD characteristics:
   âœ“ Pure I/O bound (>90%)
   âœ“ Uniform request patterns
   âœ“ No CPU-intensive operations

4. PROVEN architecture:
   âœ“ Using battle-tested framework (libuv, tokio)
   âœ“ Not building from scratch
   âœ“ Reference implementations available

Example: nginx, Redis, Node.js
â†’ Already using event-driven successfully âœ…
```

---

**HYBRID APPROACH (Advanced):**
```
Best of both worlds:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Event-driven Main Loop                â”‚
â”‚  â”œâ”€ Handle network I/O (non-blocking)  â”‚
â”‚  â””â”€ Fast path: Serve from cache       â”‚
â”‚                                        â”‚
â”‚  Thread Pool (for slow path):         â”‚
â”‚  â”œâ”€ Disk I/O operations                â”‚
â”‚  â”œâ”€ CPU-intensive processing           â”‚
â”‚  â””â”€ Blocking operations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros:
âœ… High performance (event-driven network)
âœ… No blocking (thread pool for disk)
âœ… Simpler than pure event-driven

Cons:
âŒ Even more complex
âŒ Requires careful coordination

Example: Haproxy, modern Nginx
```

---

#### **F. Báº£ng khuyáº¿n nghá»‹ cuá»‘i cÃ¹ng**

| Scenario | Recommended | Alternative | Notes |
|----------|-------------|-------------|-------|
| **Standard file service** | Multi-threaded âœ… | Event-driven | Balanced approach |
| **High load (>50K req/s)** | Event-driven âœ… | Multi-threaded + scale | If expertise available |
| **Massive connections (>100K)** | Event-driven âœ…âœ… | - | Only viable option |
| **Small team, fast delivery** | Multi-threaded âœ…âœ… | - | Lower risk |
| **Read-heavy + caching** | Multi-threaded âœ… | Event-driven | RW-locks work well |
| **Mixed workload** | Multi-threaded âœ… | - | More flexible |
| **Expert team** | Event-driven âœ… | Hybrid | Best performance |

---

### **TÃ“M Táº®T CUá»I CÃ™NG:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  KHUYáº¾N NGHá»Š CHO FILE SERVICE (READ-HEAVY)          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                      â•‘
â•‘  ğŸ† CHá»ŒN: MULTI-THREADED (Thread Pool)              â•‘
â•‘                                                      â•‘
â•‘  LÃ DO:                                              â•‘
â•‘  âœ… ÄÃ¡p á»©ng performance requirements                â•‘
â•‘  âœ… Reasonable development complexity               â•‘
â•‘  âœ… Good fault tolerance                            â•‘
â•‘  âœ… Proven, well-understood approach                â•‘
â•‘  âœ… Standard tooling and debugging                  â•‘
â•‘  âœ… Optimal cho read-heavy workload                 â•‘
â•‘                                                      â•‘
â•‘  ÄIá»‚M Máº NH:                                          â•‘
â•‘  â€¢ 10K+ req/s throughput                            â•‘
â•‘  â€¢ Reader-writer locks: minimal contention          â•‘
â•‘  â€¢ Thread isolation: good fault tolerance           â•‘
â•‘  â€¢ 8 weeks development: reasonable timeline         â•‘
â•‘                                                      â•‘
â•‘  KHI NÃ€O UPGRADE LÃŠN EVENT-DRIVEN:                  â•‘
â•‘  â€¢ Khi cáº§n scale >50K req/s                         â•‘
â•‘  â€¢ Khi cÃ³ expert team                               â•‘
â•‘  â€¢ Sau khi profile vÃ  tá»‘i Æ°u multi-threaded         â•‘
â•‘                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•