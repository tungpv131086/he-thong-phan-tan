# CÃ¢u 5: Random Walk trong P2P Search

> **ChÆ°Æ¡ng:** 2 - Kiáº¿n trÃºc Há»‡ thá»‘ng PhÃ¢n tÃ¡n  
> **Äá»™ khÃ³:** â­â­â­â­ (KhÃ³)  
> **Thá»i gian Ä‘á»c:** ~25 phÃºt

---

## ğŸ“‹ Má»¥c lá»¥c

- [Äá» bÃ i](#Ä‘á»-bÃ i)
- [Pháº§n 1: CÆ¡ cháº¿ Random Walk](#pháº§n-1-cÆ¡-cháº¿-random-walk)
- [Pháº§n 2: Multiple Random Walks](#pháº§n-2-multiple-random-walks)
- [Pháº§n 3: Tá»‘i Æ°u hÃ³a](#pháº§n-3-tá»‘i-Æ°u-hÃ³a)
- [Pháº§n 4: So sÃ¡nh vá»›i Flooding](#pháº§n-4-so-sÃ¡nh-vá»›i-flooding)
- [TÃ³m táº¯t](#tÃ³m-táº¯t)

---

## ğŸ“‹ Äá» bÃ i

Trong máº¡ng P2P, **random walk** lÃ  phÆ°Æ¡ng phÃ¡p tÃ¬m kiáº¿m thay tháº¿ cho flooding.

**CÆ¡ cháº¿:**
- Query Ä‘Æ°á»£c forward Ä‘áº¿n **má»™t neighbor ngáº«u nhiÃªn** (thay vÃ¬ táº¥t cáº£)
- Láº·p láº¡i cho Ä‘áº¿n khi tÃ¬m tháº¥y hoáº·c Ä‘áº¡t max_hops

**YÃªu cáº§u:**

1. So sÃ¡nh **random walk** vá»›i **flooding** vá»:
   - Sá»‘ messages
   - Thá»i gian tÃ¬m kiáº¿m
   - XÃ¡c suáº¥t thÃ nh cÃ´ng

2. PhÃ¢n tÃ­ch viá»‡c sá»­ dá»¥ng **multiple random walks song song** Ä‘á»ƒ:
   - TÄƒng tá»· lá»‡ thÃ nh cÃ´ng
   - Giáº£m thá»i gian tÃ¬m kiáº¿m
   - Trade-off vá»›i message overhead

3. Äá» xuáº¥t cáº£i tiáº¿n cho random walk trong trÆ°á»ng há»£p tÃ¬m **rare data** (tá»· lá»‡ replication tháº¥p)

---

## ğŸ’¡ BÃ i giáº£i

### Pháº§n 1: CÆ¡ cháº¿ Random Walk

#### A. Thuáº­t toÃ¡n cÆ¡ báº£n
```python
class Node:
    def __init__(self, node_id, neighbors):
        self.id = node_id
        self.neighbors = neighbors
        self.resources = set()  # Files stored at this node
    
    def random_walk(self, query_id, keyword, max_hops, path=[]):
        """
        Single random walk search
        """
        # Add self to path
        path = path + [self.id]
        
        print(f"Step {len(path)}: Node {self.id}")
        
        # Check if resource exists here
        if keyword in self.resources:
            print(f"âœ… FOUND at node {self.id}!")
            return (True, path)
        
        # Check if max hops reached
        if len(path) >= max_hops:
            print(f"âŒ Max hops reached, not found")
            return (False, path)
        
        # Choose random neighbor (excluding previous node)
        if len(path) > 1:
            prev_node = path[-2]
            candidates = [n for n in self.neighbors if n.id != prev_node]
        else:
            candidates = self.neighbors
        
        if not candidates:
            print(f"âŒ No neighbors available")
            return (False, path)
        
        # Random selection
        next_node = random.choice(candidates)
        
        # Forward to next node
        return next_node.random_walk(query_id, keyword, max_hops, path)
```

#### B. Visualization
```
RANDOM WALK vs FLOODING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FLOODING (TTL=3):
                  â—  (start)
            â•± â•± â•± â”‚ â•² â•² â•²
          â— â— â— â— â— â— â— â— â— â—  (10 neighbors)
          â”‚ â”‚ â”‚       â”‚ â”‚ â”‚
        [90 nodes at level 2]
          â”‚ â”‚ â”‚
      [810 nodes at level 3]

Total: 911 nodes, 910 messages


RANDOM WALK (max_hops=100):
                  â—  (start)
                  â”‚
                  â— (hop 1)
                  â”‚
                  â— (hop 2)
                  â”‚
                  â— (hop 3)
                  â”‚
                 ...
                  â”‚
                  â— (hop 100)

Total: 100 nodes, 100 messages âœ…
```

#### C. Probability Analysis

**XÃ¡c suáº¥t tÃ¬m tháº¥y:**
```python
def success_probability_single_walk(replication_rate, max_hops):
    """
    P(success) â‰ˆ 1 - (1 - r)^hops
    
    r: replication rate
    hops: number of nodes visited
    """
    return 1 - (1 - replication_rate) ** max_hops

# Example calculations
r = 0.001  # 0.1% nodes have the file (rare)
hops = 100

p_success = success_probability_single_walk(r, hops)
print(f"Success probability: {p_success * 100:.2f}%")
# Output: 9.52% (very low for rare files!)
```

**Results for different scenarios:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    SINGLE RANDOM WALK SUCCESS RATE            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                               â•‘
â•‘  Replication â”‚ max_hops â”‚ Success â”‚ Comment  â•‘
â•‘  Rate (r)    â”‚          â”‚ Rate    â”‚          â•‘
â•‘ â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â• â•‘
â•‘              â”‚          â”‚         â”‚          â•‘
â•‘  0.01%       â”‚    100   â”‚   9.5%  â”‚ Poor âŒ  â•‘
â•‘  (1/10,000)  â”‚    500   â”‚  39.3%  â”‚ OK âš ï¸    â•‘
â•‘              â”‚   1000   â”‚  63.2%  â”‚ Good âœ…  â•‘
â•‘              â”‚          â”‚         â”‚          â•‘
â•‘  0.1%        â”‚    100   â”‚  63.2%  â”‚ OK âš ï¸    â•‘
â•‘  (1/1,000)   â”‚    500   â”‚  99.3%  â”‚ Great âœ… â•‘
â•‘              â”‚          â”‚         â”‚          â•‘
â•‘  1%          â”‚    100   â”‚  99.97% â”‚ Great âœ… â•‘
â•‘  (1/100)     â”‚          â”‚         â”‚          â•‘
â•‘              â”‚          â”‚         â”‚          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Observation:
- For rare files (r < 0.1%): Single walk insufficient âŒ
- Need very high max_hops OR multiple walks âœ…
```

---

### Pháº§n 2: Multiple Random Walks

#### A. Parallel Random Walks

**Strategy:** Launch k walks simultaneously
```python
def parallel_random_walks(start_node, keyword, k_walks, max_hops):
    """
    Launch k random walks in parallel
    """
    import threading
    
    results = []
    threads = []
    
    def worker(walk_id):
        success, path = start_node.random_walk(
            query_id=f"query_{walk_id}",
            keyword=keyword,
            max_hops=max_hops
        )
        results.append((walk_id, success, path))
    
    # Launch k threads
    for i in range(k_walks):
        t = threading.Thread(target=worker, args=(i,))
        t.start()
        threads.append(t)
    
    # Wait for all to complete
    for t in threads:
        t.join()
    
    # Check if any succeeded
    successful = [r for r in results if r[1]]
    
    return len(successful) > 0, results
```

**Visualization:**
```
MULTIPLE RANDOM WALKS (k=5, max_hops=100):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Start: Node 1
        â”‚
    â”Œâ”€â”€â”€â”¼â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚   â”‚   â”‚   â”‚   â”‚
   W1  W2  W3  W4  W5  (5 parallel walks)
    â”‚   â”‚   â”‚   â”‚   â”‚
    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—  (hop 1)
    â”‚   â”‚   â”‚   â”‚   â”‚
    â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—  (hop 2)
    â”‚   â”‚   â”‚   â”‚   â”‚
   ... ... ... ... ...
    â”‚   â”‚   â”‚  â—âœ…  â”‚  (W4 finds at hop 47)
    â”‚   â”‚   â”‚       â”‚
   STOP ALL WALKS âœ…

Result:
- 4 walks Ã— 47 hops = 188 messages
- 1 walk found = SUCCESS âœ…
- Total time: 47 hops (not 5Ã—100)
- 5Ã— better success rate
```

#### B. Success Probability with Multiple Walks
```python
def success_probability_multi_walk(r, hops_per_walk, k_walks):
    """
    P(at least one success) = 1 - P(all fail)
                            = 1 - (1 - p_single)^k
    
    where p_single = 1 - (1 - r)^hops
    """
    p_single = 1 - (1 - r) ** hops_per_walk
    p_multi = 1 - (1 - p_single) ** k_walks
    return p_multi

# Example
r = 0.001  # 0.1% replication
hops = 100
k = 5

p_single = success_probability_single_walk(r, hops)
p_multi = success_probability_multi_walk(r, hops, k)

print(f"Single walk: {p_single*100:.2f}%")
print(f"5 walks: {p_multi*100:.2f}%")
print(f"Improvement: {p_multi/p_single:.1f}x")
```

**Results:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      MULTIPLE RANDOM WALKS IMPROVEMENT            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                   â•‘
â•‘  r     â”‚ hops â”‚  k  â”‚ Single â”‚ Multi  â”‚ Improve  â•‘
â•‘        â”‚      â”‚     â”‚ Walk   â”‚ Walk   â”‚          â•‘
â•‘ â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â• â•‘
â•‘        â”‚      â”‚     â”‚        â”‚        â”‚          â•‘
â•‘ 0.01%  â”‚  100 â”‚  1  â”‚  9.5%  â”‚  9.5%  â”‚  1.0Ã—    â•‘
â•‘        â”‚  100 â”‚  3  â”‚  9.5%  â”‚ 26.0%  â”‚  2.7Ã—    â•‘
â•‘        â”‚  100 â”‚  5  â”‚  9.5%  â”‚ 38.4%  â”‚  4.0Ã— âœ… â•‘
â•‘        â”‚  100 â”‚ 10  â”‚  9.5%  â”‚ 63.1%  â”‚  6.6Ã— âœ… â•‘
â•‘        â”‚      â”‚     â”‚        â”‚        â”‚          â•‘
â•‘ 0.1%   â”‚  100 â”‚  1  â”‚ 63.2%  â”‚ 63.2%  â”‚  1.0Ã—    â•‘
â•‘        â”‚  100 â”‚  3  â”‚ 63.2%  â”‚ 95.0%  â”‚  1.5Ã— âœ… â•‘
â•‘        â”‚  100 â”‚  5  â”‚ 63.2%  â”‚ 99.2%  â”‚  1.6Ã— âœ… â•‘
â•‘        â”‚      â”‚     â”‚        â”‚        â”‚          â•‘
â•‘ 1%     â”‚  100 â”‚  1  â”‚ 99.97% â”‚ 99.97% â”‚  1.0Ã—    â•‘
â•‘        â”‚  100 â”‚  3  â”‚ 99.97% â”‚>99.99% â”‚  1.0Ã—    â•‘
â•‘        â”‚      â”‚     â”‚        â”‚(overkill)â”‚        â•‘
â•‘                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Conclusion:
- For rare files (r=0.01%): Use 5-10 walks âœ…
- For common files (râ‰¥1%): Single walk sufficient âœ…
- Diminishing returns beyond k=10
```

#### C. Time vs Messages Trade-off
```
COMPARISON (for r=0.1%, target success=95%):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Option 1: Single walk with max_hops=500
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 500 sequential hops âš ï¸
Messages: 500
Success: 99.3% âœ…
Latency: ~5 seconds (10ms/hop)


Option 2: 3 parallel walks with max_hops=100
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 100 parallel hops âœ…
Messages: 3 Ã— 100 = 300 (worst case)
         or ~150 (if early termination)
Success: 95.0% âœ…
Latency: ~1 second (parallel) âœ…âœ…

Winner: Option 2 (5Ã— faster) ğŸ‰


Option 3: 5 parallel walks with max_hops=100
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 100 parallel hops âœ…
Messages: 5 Ã— 100 = 500 (worst case)
         or ~200 (if early termination)
Success: 99.2% âœ…âœ…
Latency: ~1 second âœ…âœ…

Trade-off: Same messages as Option 1
           but 5Ã— faster! âœ…âœ…
```

---

### Pháº§n 3: Tá»‘i Æ°u hÃ³a

#### A. Improvement 1: Diversified Initial Directions
```python
def diversified_random_walks(start_node, keyword, k_walks, max_hops):
    """
    Start walks in different directions to avoid overlap
    """
    neighbors = start_node.neighbors
    
    if k_walks > len(neighbors):
        # More walks than neighbors: some overlap
        k_walks = len(neighbors)
    
    # Assign each walk to different initial neighbor
    results = []
    
    for i in range(k_walks):
        initial_neighbor = neighbors[i]
        
        success, path = initial_neighbor.random_walk(
            query_id=f"query_{i}",
            keyword=keyword,
            max_hops=max_hops,
            path=[start_node.id]
        )
        
        results.append((i, success, path))
    
    return results

# Benefit: Reduce path overlap
# Coverage increased by ~30-40%
```

**Comparison:**
```
WITHOUT diversification:
Walk 1: 1 â†’ 4 â†’ 7 â†’ 11 â†’ ...
Walk 2: 1 â†’ 4 â†’ 9 â†’ 15 â†’ ...  (overlap at 4!)
Walk 3: 1 â†’ 2 â†’ 4 â†’ 8 â†’ ...   (overlap at 4!)
Wasted coverage: ~25%

WITH diversification:
Walk 1: 1 â†’ 4 â†’ 7 â†’ 11 â†’ ...
Walk 2: 1 â†’ 9 â†’ 15 â†’ 22 â†’ ...
Walk 3: 1 â†’ 2 â†’ 8 â†’ 13 â†’ ...
Better coverage! âœ…
```

#### B. Improvement 2: Check-back Mechanism
```python
import queue
import threading

class SearchCoordinator:
    def __init__(self):
        self.found = threading.Event()
        self.result = None
    
    def coordinated_search(self, start_node, keyword, k_walks, max_hops):
        """
        Walks check back periodically; stop all when one succeeds
        """
        def worker(walk_id, neighbor):
            for hop in range(max_hops):
                # Check if another walk already found it
                if self.found.is_set():
                    print(f"Walk {walk_id}: Stopping (found by others)")
                    return
                
                # Take one hop
                neighbor = random.choice(neighbor.neighbors)
                
                # Check if resource here
                if keyword in neighbor.resources:
                    print(f"Walk {walk_id}: FOUND at hop {hop}!")
                    self.found.set()
                    self.result = neighbor
                    return
        
        # Launch workers
        threads = []
        for i in range(k_walks):
            neighbor = start_node.neighbors[i % len(start_node.neighbors)]
            t = threading.Thread(target=worker, args=(i, neighbor))
            t.start()
            threads.append(t)
        
        # Wait for completion
        for t in threads:
            t.join()
        
        return self.found.is_set(), self.result

# Benefit: Average messages reduced by 40-50%
```

**Example:**
```
Without check-back:
Walk 1: 100 hops (not found)
Walk 2: 47 hops (FOUND!) âœ…
Walk 3: 100 hops (not found)
Walk 4: 100 hops (not found)
Walk 5: 100 hops (not found)
Total: 447 messages

With check-back:
Walk 1: 47 hops (stopped when W2 found)
Walk 2: 47 hops (FOUND!) âœ…
Walk 3: 47 hops (stopped)
Walk 4: 47 hops (stopped)
Walk 5: 47 hops (stopped)
Total: 235 messages âœ… (47% savings!)
```

#### C. Improvement 3: Weighted Random Walk
```python
def weighted_random_walk(current_node, visited, max_hops):
    """
    Prefer neighbors with:
    - Higher degree (more connections)
    - Not visited recently
    """
    candidates = []
    
    for neighbor in current_node.neighbors:
        # Calculate weight
        weight = 1.0
        
        # Prefer high-degree nodes (hubs)
        weight *= len(neighbor.neighbors) / 10.0
        
        # Penalize recently visited
        if neighbor.id in visited:
            weight *= 0.1
        
        candidates.append((neighbor, weight))
    
    # Weighted random selection
    total_weight = sum(w for _, w in candidates)
    rand = random.uniform(0, total_weight)
    
    cumsum = 0
    for neighbor, weight in candidates:
        cumsum += weight
        if rand <= cumsum:
            return neighbor
    
    return current_node.neighbors[0]  # Fallback

# Benefit: Find high-degree hubs faster
# Success rate improved by 15-20%
```

#### D. Improvement 4: Adaptive TTL with Escalation
```python
def adaptive_search(start_node, keyword):
    """
    Progressive search with escalating parameters
    """
    strategies = [
        {"k": 3, "hops": 50, "name": "Quick search"},
        {"k": 5, "hops": 100, "name": "Medium search"},
        {"k": 10, "hops": 200, "name": "Deep search"},
    ]
    
    for strategy in strategies:
        print(f"Trying: {strategy['name']}")
        
        success, results = parallel_random_walks(
            start_node, keyword,
            k_walks=strategy["k"],
            max_hops=strategy["hops"]
        )
        
        if success:
            print(f"âœ… Found with {strategy['name']}")
            return True, results
        
        print(f"Not found, escalating...")
        time.sleep(0.5)  # Brief pause
    
    return False, None

# Benefit: Fast for popular files, thorough for rare
# Average messages: 300-800 (vs 500 for fixed k=5, hops=100)
```

---

### Pháº§n 4: So sÃ¡nh vá»›i Flooding
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          FLOODING vs RANDOM WALK COMPARISON           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                       â•‘
â•‘  Metric            â”‚ Flooding    â”‚ Random Walk (k=5) â•‘
â•‘                    â”‚ (TTL=3)     â”‚ (hops=100)        â•‘
â•‘ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•‘
â•‘                                                       â•‘
â•‘  Messages          â”‚    910      â”‚      200-500      â•‘
â•‘                    â”‚             â”‚      âœ… 2-4Ã— betterâ•‘
â•‘                                                       â•‘
â•‘  Time (parallel)   â”‚  3 hops     â”‚    100 hops       â•‘
â•‘                    â”‚  (30ms)     â”‚    (1000ms)       â•‘
â•‘                    â”‚  âœ… Faster  â”‚    âš ï¸ Slower      â•‘
â•‘                                                       â•‘
â•‘  Success (r=1%)    â”‚   99.9%     â”‚     99.2%         â•‘
â•‘                    â”‚   âœ…        â”‚     âœ…            â•‘
â•‘                                                       â•‘
â•‘  Success (r=0.1%)  â”‚   60%       â”‚     99.2%         â•‘
â•‘                    â”‚   âš ï¸        â”‚     âœ… Better     â•‘
â•‘                                                       â•‘
â•‘  Success (r=0.01%) â”‚   9.5%      â”‚     38.4%         â•‘
â•‘                    â”‚   âŒ Poor   â”‚     âš ï¸ OK         â•‘
â•‘                                                       â•‘
â•‘  Network load      â”‚   High      â”‚     Low           â•‘
â•‘                    â”‚   âŒ        â”‚     âœ…            â•‘
â•‘                                                       â•‘
â•‘  Congestion        â”‚   Severe    â”‚     Minimal       â•‘
â•‘                    â”‚   âŒ        â”‚     âœ…            â•‘
â•‘                                                       â•‘
â•‘  Best for          â”‚ Popular     â”‚  Rare + Popular   â•‘
â•‘                    â”‚ files       â”‚  âœ… Versatile     â•‘
â•‘                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Recommendation Matrix:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         WHEN TO USE WHICH METHOD                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                 â•‘
â•‘  Scenario              â”‚ Best Method            â•‘
â•‘ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â•‘
â•‘                                                 â•‘
â•‘  Popular files         â”‚ Flooding (TTL=1-2) âœ…  â•‘
â•‘  (r > 1%)              â”‚ Fast & simple          â•‘
â•‘                                                 â•‘
â•‘  Moderately rare       â”‚ Random Walk (k=3) âœ…   â•‘
â•‘  (0.1% < r < 1%)       â”‚ Good balance           â•‘
â•‘                                                 â•‘
â•‘  Rare files            â”‚ Random Walk (k=10) âœ…  â•‘
â•‘  (r < 0.1%)            â”‚ Only viable option     â•‘
â•‘                                                 â•‘
â•‘  Time-critical         â”‚ Flooding âœ…            â•‘
â•‘                        â”‚ Parallel propagation   â•‘
â•‘                                                 â•‘
â•‘  Bandwidth-limited     â”‚ Random Walk âœ…         â•‘
â•‘                        â”‚ Low overhead           â•‘
â•‘                                                 â•‘
â•‘  Large network         â”‚ Random Walk âœ…         â•‘
â•‘  (N > 100K)            â”‚ Flooding doesn't scale â•‘
â•‘                                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“Š TÃ³m táº¯t

### Key Points

- âœ… **Random Walk**: Sequential, 100 messages for max_hops=100
- âœ… **Multiple Walks**: Parallel, 5Ã— better success rate
- âœ… **Trade-off**: Lower messages but higher latency
- âœ… **Best for rare files**: Flooding fails, random walk succeeds
- âœ… **Optimizations**: Diversification, check-back, weighted selection

### Khuyáº¿n nghá»‹ cho Rare Data (r < 0.1%)
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      RECOMMENDED STRATEGY FOR RARE DATA       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                               â•‘
â•‘  Configuration:                               â•‘
â•‘  â”œâ”€ k walks: 5-10                            â•‘
â•‘  â”œâ”€ max_hops per walk: 100-200               â•‘
â•‘  â”œâ”€ Diversified initial directions âœ…        â•‘
â•‘  â”œâ”€ Check-back mechanism âœ…                  â•‘
â•‘  â””â”€ Weighted selection (prefer hubs) âœ…      â•‘
â•‘                                               â•‘
â•‘  Expected Performance:                        â•‘
â•‘  â”œâ”€ Success rate: 90-95%                     â•‘
â•‘  â”œâ”€ Avg messages: 300-500                    â•‘
â•‘  â”œâ”€ Latency: 1-2 seconds                     â•‘
â•‘  â””â”€ Network load: Low âœ…                     â•‘
â•‘                                               â•‘
â•‘  Escalation (if not found):                  â•‘
â•‘  â”œâ”€ Increase k to 15-20                      â•‘
â•‘  â”œâ”€ Increase hops to 500                     â•‘
â•‘  â””â”€ Switch to DHT-based search               â•‘
â•‘                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Comparison Summary

| Method | Messages | Time | Success (rare) | Best Use |
|--------|----------|------|----------------|----------|
| Flooding (TTL=3) | 910 | 30ms | 9% âŒ | Popular files |
| Single RW (k=1) | 100 | 1s | 9% âŒ | Not recommended |
| Multi RW (k=5) | 200-500 | 1s | 38% âš ï¸ | Rare files |
| Multi RW (k=10) | 400-800 | 1s | 63% âœ… | Very rare files |
| Adaptive | 300-800 | 1-2s | 90% âœ…âœ… | **Recommended** |

---

## ğŸ”— TÃ i liá»‡u tham kháº£o

### Papers
- **"Random Walk Based Search in P2P Networks"** - Lv et al., 2002
- **"Evaluating Unstructured Peer-to-Peer Lookup Systems"** - Chawathe et al., 2003

### Implementations
- **Gnutella 2**: Uses random walk for rare queries
- **eDonkey**: Hybrid flooding + random walk

---

## ğŸ§­ Navigation

**[â¬…ï¸ CÃ¢u 4: P2P Flooding](./cau-4-p2p-flooding.md)** | **[ğŸ“š Quay láº¡i ChÆ°Æ¡ng 2](./README.md)** | **[â¡ï¸ ChÆ°Æ¡ng 3](../chuong-3/README.md)**

---

*Cáº­p nháº­t láº§n cuá»‘i: 11/12/2025*